{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EMNIST Transfer 2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D3C0IQXGgLT"
      },
      "source": [
        "# Character recognition with transfer learning in PyTorch\n",
        "\n",
        "This is an example of using transfer learning on a character recognition task.\n",
        "We used the pretrained resnet18 architecture to predict on the EMNIST database, which achieves an accuracy of ~%82."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFJ7nnwO1kBF"
      },
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "import torchvision.models as models"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfdHj2580xk7"
      },
      "source": [
        "#Download the pretrained resnet18 model\n",
        "resnet18 = models.resnet18(pretrained=True)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asnNd3Ax0hiT"
      },
      "source": [
        "preprocess = torchvision.transforms.Compose([\n",
        "                      torchvision.transforms.ToTensor(),\n",
        "                      \n",
        "                      torchvision.transforms.Lambda(lambda x: torch.cat([x, x, x], 0)), # Transform the 1 channel imges to 3 channel\n",
        "                    \n",
        "                      torchvision.transforms.Lambda(lambda x: torch.transpose(x,1,2)), # Transpose the letters so that they are human readable\n",
        "                      \n",
        "])\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJN-L94H0sdT",
        "outputId": "b775a6a6-0f75-4c5d-f7db-fc01ea8bd318"
      },
      "source": [
        "from torchvision import datasets\n",
        "ds_train = datasets.EMNIST(root='./emnist_data/', split = 'letters', train=True, transform=preprocess, download=True)\n",
        "ds_test = datasets.EMNIST(root='./emnist_data/', split = 'letters', train=False, transform=preprocess, download=True)\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "dl_train = DataLoader(ds_train,batch_size=200,shuffle=True)\n",
        "dl_test = DataLoader(ds_test,batch_size=200,shuffle=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOAouoQo0tRh"
      },
      "source": [
        "# freeze the layers of the resnet model \n",
        "resnet2 = resnet18\n",
        "\n",
        "for param in resnet18.parameters():\n",
        "  param.requires_grad = False"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCX4VOu-2f5X"
      },
      "source": [
        "from torch import nn, optim\n",
        "\n",
        "# The model is fully convolutional in order to be generalized and be used in the future on \n",
        "# handwriting analysis tasks\n",
        "\n",
        "class Transfer2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Transfer2, self).__init__()\n",
        "        \n",
        "        self.resnet = resnet2\n",
        "        self.relu = nn.ReLU()\n",
        "        self.avgpool = nn.AdaptiveMaxPool2d((1,1))\n",
        "        self.hidden1 = nn.Linear(128,64)\n",
        "        # self.hidden2 = nn.Linear(100,64)\n",
        "        self.output = nn.Linear(64,26)\n",
        "        self.logsoftmax = nn.Softmax(dim=1)\n",
        "        self.dropout = nn.Dropout(0.15)\n",
        "    \n",
        "    def forward(self,x):\n",
        "        x = self.resnet.conv1(x)\n",
        "        x = self.resnet.bn1(x)\n",
        "        x = self.resnet.relu(x)\n",
        "        x = self.resnet.maxpool(x)\n",
        "        x = self.resnet.layer1(x)\n",
        "        x = self.resnet.layer2(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(-1,128)\n",
        "        #x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.hidden1(x)\n",
        "        # x = self.hidden2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.output(x)\n",
        "        x = self.logsoftmax(x)\n",
        "        return x\n",
        "\n",
        "# n_classes = 27"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7TJuJL62lDb",
        "outputId": "759beb5b-3020-41c6-c34a-e1a81b4bca3f"
      },
      "source": [
        "from torch import nn, optim\n",
        "net = Transfer2()\n",
        "\n",
        "# x = DataLoader(ds,batch_size=200,shuffle=True)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "optimizer = optim.Adam(net.parameters())\n",
        "\n",
        "#function to turn integers into one-hot vectors\n",
        "def hotvecs(batch,n_classes):\n",
        "  out = torch.zeros((batch.shape[0],n_classes),dtype=torch.float32)\n",
        "  ids2 = torch.arange(batch.shape[0])\n",
        "  out[ids2,batch] = 1.\n",
        "  return out\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(3):\n",
        "  for i,(train_features,train_labels) in enumerate(dl_train):\n",
        "    optimizer.zero_grad()\n",
        "    out = net(train_features)\n",
        "    loss = criterion(out,hotvecs(train_labels-1,26))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if i%100 == 0:\n",
        "      print(loss,epoch)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0370, grad_fn=<MseLossBackward>) 0\n",
            "tensor(0.0323, grad_fn=<MseLossBackward>) 0\n",
            "tensor(0.0233, grad_fn=<MseLossBackward>) 0\n",
            "tensor(0.0179, grad_fn=<MseLossBackward>) 0\n",
            "tensor(0.0170, grad_fn=<MseLossBackward>) 0\n",
            "tensor(0.0126, grad_fn=<MseLossBackward>) 0\n",
            "tensor(0.0142, grad_fn=<MseLossBackward>) 0\n",
            "tensor(0.0122, grad_fn=<MseLossBackward>) 1\n",
            "tensor(0.0124, grad_fn=<MseLossBackward>) 1\n",
            "tensor(0.0098, grad_fn=<MseLossBackward>) 1\n",
            "tensor(0.0103, grad_fn=<MseLossBackward>) 1\n",
            "tensor(0.0113, grad_fn=<MseLossBackward>) 1\n",
            "tensor(0.0107, grad_fn=<MseLossBackward>) 1\n",
            "tensor(0.0127, grad_fn=<MseLossBackward>) 1\n",
            "tensor(0.0119, grad_fn=<MseLossBackward>) 2\n",
            "tensor(0.0090, grad_fn=<MseLossBackward>) 2\n",
            "tensor(0.0116, grad_fn=<MseLossBackward>) 2\n",
            "tensor(0.0108, grad_fn=<MseLossBackward>) 2\n",
            "tensor(0.0086, grad_fn=<MseLossBackward>) 2\n",
            "tensor(0.0115, grad_fn=<MseLossBackward>) 2\n",
            "tensor(0.0119, grad_fn=<MseLossBackward>) 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "iXh709mT4KCW",
        "outputId": "789441a8-982e-443e-dfeb-289753d0fed8"
      },
      "source": [
        "# validate on training data\n",
        "train_features, train_labels = next(iter(dl_test))\n",
        "z = net(train_features)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "A = confusion_matrix(train_labels-1,torch.argmax(z,axis=1))\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "plt.imshow(A)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print('accuracy = ',torch.sum((train_labels-1)==torch.argmax(z,axis=1))/200)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy =  tensor(0.8500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVGAl2EL6uGV"
      },
      "source": [
        "right = 0\n",
        "total = 0\n",
        "for train_features, train_labels in dl_test:\n",
        "  z = net(train_features)\n",
        "  correct = torch.sum((train_labels-1)==torch.argmax(z,axis=1))\n",
        "  right+=correct \n",
        "  total+=len(train_labels)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Od1KKQvqI3e4",
        "outputId": "111d582b-e50f-49e3-cdf8-4a812bf3857a"
      },
      "source": [
        "print('test accuracy = %.3f'%float(right/total))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy = 0.826\n"
          ]
        }
      ]
    }
  ]
}